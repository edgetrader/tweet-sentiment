{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Prediction\n",
    "- by Chee-Foong\n",
    "- Apr 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This analysis attempts to train a couple of machine learning models to predict the sentiment of tweets.\n",
    "\n",
    "## Data\n",
    "\n",
    "Using data downloaded from Kaggle competition: [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction/data)\n",
    "\n",
    "## Scope\n",
    "\n",
    "Note that the objective of the Kaggle is to predict the list of keywords that is relevant for the sentiment of each tweet.  The work here is <span style=\"color:red\">**NOT**</span> an analysis for the competition.  I am just using the tweets and the respective sentiments given in the train and test data set of explore various machine learning models and NLP libraries / techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Download data files from Kaggle\n",
    "Reference code how to download and extract files from Kaggle.  Not required once files are downloaded.  Files are stored in directory <span style=\"color:red\">**../data**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle --upgrade\n",
    "# !cp ../utility/kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle competitions download -c tweet-sentiment-extraction\n",
    "# !mv *.zip ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# datafolder = '../data/'\n",
    "\n",
    "# file_list = []\n",
    "# dir_list = []\n",
    "# for root, dirs, files in os.walk(datafolder, topdown=False):\n",
    "#     for name in files:\n",
    "#         if (name != '.DS_Store'):\n",
    "#             file_list.append(os.path.join(datafolder, name))\n",
    "#     for name in dirs:\n",
    "#         dir_list.append(os.path.join(datafolder, name))\n",
    "\n",
    "# for file in file_list:\n",
    "#     if file.endswith('.zip'):\n",
    "#         zip_ref = zipfile.ZipFile(file, 'r')\n",
    "#         zip_ref.extractall(datafolder)\n",
    "#         zip_ref.close()\n",
    "#         os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## Loading Language Model\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7fa821dff190>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fa821e021c0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fa821e02700>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.append('../src') \n",
    "\n",
    "from cleandata import *\n",
    "from memory import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Ingestion and Exploratory\n",
    "\n",
    "Notes\n",
    "- Sample_submission.csv is not relevant for the analysis and will not be loaded.\n",
    "- 'selected_text' column in train.csv is not relevant here and will be dropped.\n",
    "- 'textID' will remain for referencing later if required\n",
    "- test.csv will serve as the out of sample data set.\n",
    "- 'sentiment' column in test.csv will be used to measure model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission = pd.read_csv(datafolder + \"sample_submission.csv\")\n",
    "test = pd.read_csv(datafolder + \"test.csv\")\n",
    "train = pd.read_csv(datafolder + \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27481, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['selected_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['textID', 'text', 'sentiment'], dtype='object')\n",
      "Index(['textID', 'text', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping invalid text records from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "textID       0\n",
      "text         1\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_cat = train.select_dtypes(include=['object']).copy()\n",
    "print(train_cat.isnull().values.sum())\n",
    "print(train_cat.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>fdb77c3752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID text sentiment\n",
       "314  fdb77c3752  NaN   neutral"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "textID       0\n",
      "text         0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_cat = test.select_dtypes(include=['object']).copy()\n",
    "print(test_cat.isnull().values.sum())\n",
    "print(test_cat.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_cat\n",
    "del test_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Cleanising and Feature Engineering\n",
    "\n",
    "Generate relevant social media data\n",
    "- add number of urls in tweets\n",
    "- add number of mentions in tweets\n",
    "- add number of mentions in tweets\n",
    "\n",
    "Clean up 'text' column\n",
    "- remove url links\n",
    "- clean up short forms and abbreviations\n",
    "- remove punctuations\n",
    "\n",
    "Apply all above functions to both train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "def add_socialinfo(df, ref_column):\n",
    "    df[['url','mention','hashtag']] = df[ref_column].apply(socialinfo)\n",
    "    return df\n",
    "\n",
    "\n",
    "def socialinfo(text):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    like_url = [{\"LIKE_URL\": True}]\n",
    "    like_mention = [{\"TEXT\": {\"REGEX\": \"^[\\@][A-z0-9]+$\"}}]\n",
    "    like_hashtag = [{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}]\n",
    "\n",
    "    matcher.add(\"url\", None, like_url)\n",
    "    matcher.add(\"mention\", None, like_mention)\n",
    "    matcher.add(\"hashtag\", None, like_hashtag)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    url_count = 0\n",
    "    mention_count = 0\n",
    "    hashtag_count = 0\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        \n",
    "        if string_id == 'url':\n",
    "            url_count += 1\n",
    "        elif string_id == 'mention':\n",
    "            mention_count += 1\n",
    "        elif string_id == 'hashtag':\n",
    "            hashtag_count += 1\n",
    "            hashtag_text = doc[start:end].text\n",
    "            \n",
    "    return pd.Series([url_count, mention_count, hashtag_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Function to remove URLs\n",
    "def remove_http(tweet):\n",
    "    words = tweet.split()\n",
    "    out = [word for word in words if not word.startswith('http://')]\n",
    "    return ' '.join(out)\n",
    "\n",
    "def remove_punc(tweet):\n",
    "    punctuations = string.punctuation\n",
    "    words = tweet.split()\n",
    "    out = [ word for word in words if word not in punctuations ]\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_feature_engineering(df_train, df_test):\n",
    "    to_drop = [x for x in df_train.columns if x not in df_test.columns]\n",
    "    full = df_train.append(df_test, sort=False)\n",
    "\n",
    "    full = add_socialinfo(full, 'text')\n",
    "\n",
    "    full['text'] = full['text'].apply(remove_http)\n",
    "    full['text'] = full['text'].apply(clean)\n",
    "    full['text'] = full['text'].apply(remove_punc)\n",
    "\n",
    "    full['text'] = full['text'].str.lower()\n",
    "    \n",
    "    train = full.iloc[0:len(df_train)]\n",
    "    test = full.iloc[len(df_train):].drop(to_drop, axis=1)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**WARNING**</span>: The following cell may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 41s, sys: 416 ms, total: 3min 41s\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, test = text_feature_engineering(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'sentiment'\n",
    "categorical = []\n",
    "numerical = ['url', 'mention', 'hashtag']\n",
    "to_drop = ['textID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.05 MB\n",
      "Memory usage after optimization is: 0.03 MB\n",
      "Decreased by 43.8%\n",
      "Memory usage of dataframe is 0.16 MB\n",
      "Memory usage after optimization is: 0.09 MB\n",
      "Decreased by 43.8%\n"
     ]
    }
   ],
   "source": [
    "train = train.drop(to_drop, axis=1)\n",
    "test = test.drop(to_drop, axis=1)\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "\n",
    "train_wide_x = train.drop([target], axis=1)\n",
    "train_wide_y = train[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Processing for Modeling\n",
    "- Train-Test split with a ratio of 9:1\n",
    "- Tokenising and Vectorising the 'text' column\n",
    "- Replacing 'text' with tokens columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a tokeniser with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# nlp = spacy.load('en')\n",
    "\n",
    "# Create our list of stopwords\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# parser = English()\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "#     mytokens = [ word.lower_.strip() for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>Due to memory issues, I was not able to Ngram 2 or more here.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899, 4)\n",
      "(899, 2225)\n"
     ]
    }
   ],
   "source": [
    "NGRAM = 1\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_wide_x, train_wide_y, test_size=0.1, \n",
    "                                                    random_state=42, stratify=train_wide_y)\n",
    "\n",
    "print(train_X.shape)\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,NGRAM))\n",
    "# vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "train_vec_X = vectorizer.fit_transform(train_X['text'])\n",
    "test_vec_X = vectorizer.transform(test_X['text'])\n",
    "\n",
    "print(train_vec_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare input dataframe for model.  Combining all transformed dataframes together\n",
    "def prepare_df_final(df, mat_vec, drop_col):\n",
    "    df_dropped = df.drop(drop_col, axis = 1)\n",
    "    df_vec = pd.DataFrame(mat_vec.toarray()).set_index(df_dropped.index)\n",
    "    final = pd.concat([df_dropped, df_vec], axis = 1)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 111 ms, sys: 19.2 ms, total: 130 ms\n",
      "Wall time: 128 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_X = prepare_df_final(train_X, train_vec_X, ['text'])\n",
    "test_X = prepare_df_final(test_X, test_vec_X, ['text'])\n",
    "\n",
    "del train_wide_x\n",
    "del train_wide_y\n",
    "del train_vec_X\n",
    "del test_vec_X\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "## Important to manage memory usage\n",
    "train_X = scipy.sparse.csr_matrix(train_X.values)\n",
    "test_X = scipy.sparse.csr_matrix(test_X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks to ensure shape of datasets are consistent and within expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899, 2228)\n",
      "(100, 2228)\n",
      "(899,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a generic function to train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def fitModel(model, train_X, test_X, train_y, test_y):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Model: {}\".format(type(model).__name__))\n",
    "\n",
    "    if type(model).__name__ == 'XGBClassifier':\n",
    "        model.fit(train_X.toarray(), train_y)\n",
    "    else:\n",
    "        model.fit(train_X, train_y)\n",
    "    \n",
    "    # Print accuracy, time and number of dimensions\n",
    "    print(\"The program took %.3f seconds to complete.\" % (time.time() - start_time))\n",
    "\n",
    "    # Cross validation score\n",
    "#     f1score = cross_val_score(model, train_X, train_y, cv=10, scoring=\"f1\")\n",
    "#     print(f1score)\n",
    "#     print(\"The 10-fold cross validation f1 score is %.3f\" % f1score)    \n",
    "    \n",
    "    # Measure the accuracy\n",
    "    accuracy = model.score(test_X, test_y)\n",
    "    print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "    \n",
    "    return model, accuracy\n",
    "\n",
    "models = []\n",
    "ACCURACY_THRESHOLD = 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training based on list of models below with default setting.  No tuning at this stage.  Objective is to compare how each model performs against one another.\n",
    "\n",
    "   1. Naive Bayes Model\n",
    "   2. Ridge Classifier\n",
    "   3. Logistic Regression\n",
    "   4. XGBoost\n",
    "   5. Random Forest\n",
    "   6. Decision Tree\n",
    "   7. Gradient Boosting\n",
    "   8. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MultinomialNB\n",
      "The program took 0.008 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb, accuracy = fitModel(MultinomialNB(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RidgeClassifier\n",
      "The program took 0.027 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.620\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "rc, accuracy = fitModel(RidgeClassifier(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "The program took 0.077 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr, accuracy = fitModel(LogisticRegression(random_state = 1), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "<span style=\"color:red\">This may take hours to run</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBClassifier\n",
      "The program took 10.305 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.630\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb, accuracy = fitModel(XGBClassifier(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForestClassifier\n",
      "The program took 0.453 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.680\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc, accuracy = fitModel(RandomForestClassifier(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DecisionTreeClassifier\n",
      "The program took 0.066 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc, accuracy = fitModel(DecisionTreeClassifier(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(dtc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GradientBoostingClassifier\n",
      "The program took 0.774 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.620\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc, accuracy = fitModel(GradientBoostingClassifier(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVC\n",
      "The program took 0.092 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc, accuracy = fitModel(SVC(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models that meet minimum accuracy threshold: 1\n"
     ]
    }
   ],
   "source": [
    "print('Number of models that meet minimum accuracy threshold: {}'.format(len(models)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Transformation for Test Dataset\n",
    "- Tokenising and Vectorising the 'text' column\n",
    "- Replacing 'text' with tokens columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec_text = vectorizer.transform(test['text'])\n",
    "test_final = prepare_df_final(test.drop(['sentiment'], axis=1), test_vec_text, ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3534, 2228)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final = scipy.sparse.csr_matrix(test_final.values)\n",
    "test_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Prediction and Results\n",
    "Show details of all models that will be used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "predictions = []\n",
    "for model in models:\n",
    "    try:\n",
    "        if type(model).__name__ == 'XGBClassifier':\n",
    "            prediction = model.predict(test_final.toarray())\n",
    "        else:\n",
    "            prediction = model.predict(test_final)      \n",
    "        predictions.append(prediction)\n",
    "    except:\n",
    "        print(model)\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respective Model Results - Weighted F1 Score\n",
    "Random Forest Classifier performs best on out of sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compare' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-9ad80f376357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compare' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "results = list(zip(models, predictions))\n",
    "\n",
    "f1 = {}\n",
    "for result in results:\n",
    "    model = type(result[0]).__name__\n",
    "    score = f1_score(compare.sentiment, result[1], labels=labels, average='weighted')\n",
    "    f1[model] = score\n",
    "    \n",
    "pd.DataFrame([f1], index=['Weighted_F1']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling the models\n",
    "Aggregating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = pd.DataFrame(predictions)\n",
    "predict_results = pd.Series(predict_results.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = test.copy()\n",
    "compare['predicted'] = predict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([f1_score(compare.sentiment, compare.predicted, average='weighted')], index=['Weighted_F1'], columns=['Ensemble'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix over the true (rows), predicted (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(compare.sentiment)\n",
    "cm =  confusion_matrix(compare.sentiment, compare.predicted, labels=labels)\n",
    "\n",
    "pd.DataFrame(cm, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score of respective labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([f1_score(compare.sentiment, compare.predicted, average=None, labels=labels)], index=['F1'], columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
