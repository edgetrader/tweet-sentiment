{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Tweet Sentiment\n",
    "- by Chee-Foong\n",
    "- Apr 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This analysis attempts to train a couple of machine learning models to predict the sentiment of tweets.\n",
    "\n",
    "## Data\n",
    "\n",
    "Using data downloaded from Kaggle competition: [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction/data)\n",
    "\n",
    "## Scope\n",
    "\n",
    "Note that the objective of the Kaggle is to predict the list of keywords that is relevant for the sentiment of each tweet.  The work here is <span style=\"color:red\">**NOT**</span> an analysis for the competition.  I am just using the tweets and the respective sentiments given in the train and test data set of explore various machine learning models and NLP libraries / techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Download data files from Kaggle\n",
    "Reference code how to download and extract files from Kaggle.  Not required once files are downloaded.  Files are stored in directory <span style=\"color:red\">**../data**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle --upgrade\n",
    "# !cp ../utility/kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle competitions download -c tweet-sentiment-extraction\n",
    "# !mv *.zip ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# datafolder = '../data/'\n",
    "\n",
    "# file_list = []\n",
    "# dir_list = []\n",
    "# for root, dirs, files in os.walk(datafolder, topdown=False):\n",
    "#     for name in files:\n",
    "#         if (name != '.DS_Store'):\n",
    "#             file_list.append(os.path.join(datafolder, name))\n",
    "#     for name in dirs:\n",
    "#         dir_list.append(os.path.join(datafolder, name))\n",
    "\n",
    "# for file in file_list:\n",
    "#     if file.endswith('.zip'):\n",
    "#         zip_ref = zipfile.ZipFile(file, 'r')\n",
    "#         zip_ref.extractall(datafolder)\n",
    "#         zip_ref.close()\n",
    "#         os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## Loading Language Model\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7fb113f45400>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fb113f473a0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fb113f478e0>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.append('../src') \n",
    "\n",
    "from cleandata import *\n",
    "from memory import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Ingestion and Exploratory\n",
    "\n",
    "Notes\n",
    "- Sample_submission.csv is not relevant for the analysis and will not be loaded.\n",
    "- 'selected_text' column in train.csv is not relevant here and will be dropped.\n",
    "- 'textID' will remain for referencing later if required\n",
    "- test.csv will serve as the out of sample data set.\n",
    "- 'sentiment' column in test.csv will be used to measure model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission = pd.read_csv(datafolder + \"sample_submission.csv\")\n",
    "test = pd.read_csv(datafolder + \"test.csv\")\n",
    "train = pd.read_csv(datafolder + \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27481, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['selected_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['textID', 'text', 'sentiment'], dtype='object')\n",
      "Index(['textID', 'text', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping invalid text records from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "textID       0\n",
      "text         1\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_cat = train.select_dtypes(include=['object']).copy()\n",
    "print(train_cat.isnull().values.sum())\n",
    "print(train_cat.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>fdb77c3752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID text sentiment\n",
       "314  fdb77c3752  NaN   neutral"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "textID       0\n",
      "text         0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_cat = test.select_dtypes(include=['object']).copy()\n",
    "print(test_cat.isnull().values.sum())\n",
    "print(test_cat.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_cat\n",
    "del test_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Cleanising and Feature Engineering\n",
    "\n",
    "Generate relevant social media data\n",
    "- add number of urls in tweets\n",
    "- add number of mentions in tweets\n",
    "- add number of mentions in tweets\n",
    "\n",
    "Clean up 'text' column\n",
    "- remove url links\n",
    "- clean up short forms and abbreviations\n",
    "- remove punctuations\n",
    "\n",
    "Apply all above functions to both train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "def add_socialinfo(df, ref_column):\n",
    "    df[['url','mention','hashtag']] = df[ref_column].apply(socialinfo)\n",
    "    return df\n",
    "\n",
    "\n",
    "def socialinfo(text):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    like_url = [{\"LIKE_URL\": True}]\n",
    "    like_mention = [{\"TEXT\": {\"REGEX\": \"^[\\@][A-z0-9]+$\"}}]\n",
    "    like_hashtag = [{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}]\n",
    "\n",
    "    matcher.add(\"url\", None, like_url)\n",
    "    matcher.add(\"mention\", None, like_mention)\n",
    "    matcher.add(\"hashtag\", None, like_hashtag)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    url_count = 0\n",
    "    mention_count = 0\n",
    "    hashtag_count = 0\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        \n",
    "        if string_id == 'url':\n",
    "            url_count += 1\n",
    "        elif string_id == 'mention':\n",
    "            mention_count += 1\n",
    "        elif string_id == 'hashtag':\n",
    "            hashtag_count += 1\n",
    "            hashtag_text = doc[start:end].text\n",
    "            \n",
    "    return pd.Series([url_count, mention_count, hashtag_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Function to remove URLs\n",
    "def remove_http(tweet):\n",
    "    words = tweet.split()\n",
    "    out = [word for word in words if not word.startswith('http://')]\n",
    "    return ' '.join(out)\n",
    "\n",
    "def remove_punc(tweet):\n",
    "    punctuations = string.punctuation\n",
    "    words = tweet.split()\n",
    "    out = [ word for word in words if word not in punctuations ]\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_feature_engineering(df_train, df_test):\n",
    "    to_drop = [x for x in df_train.columns if x not in df_test.columns]\n",
    "    full = df_train.append(df_test, sort=False)\n",
    "\n",
    "    full = add_socialinfo(full, 'text')\n",
    "\n",
    "    full['text'] = full['text'].apply(remove_http)\n",
    "    full['text'] = full['text'].apply(clean)\n",
    "    full['text'] = full['text'].apply(remove_punc)\n",
    "\n",
    "    full['text'] = full['text'].str.lower()\n",
    "    \n",
    "    train = full.iloc[0:len(df_train)]\n",
    "    test = full.iloc[len(df_train):].drop(to_drop, axis=1)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**WARNING**</span>: The following cell may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35min 25s, sys: 9.3 s, total: 35min 34s\n",
      "Wall time: 35min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, test = text_feature_engineering(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'sentiment'\n",
    "categorical = []\n",
    "numerical = ['url', 'mention', 'hashtag']\n",
    "to_drop = ['textID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1.26 MB\n",
      "Memory usage after optimization is: 0.71 MB\n",
      "Decreased by 43.8%\n",
      "Memory usage of dataframe is 0.16 MB\n",
      "Memory usage after optimization is: 0.09 MB\n",
      "Decreased by 43.8%\n"
     ]
    }
   ],
   "source": [
    "train = train.drop(to_drop, axis=1)\n",
    "test = test.drop(to_drop, axis=1)\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "\n",
    "train_wide_x = train.drop([target], axis=1)\n",
    "train_wide_y = train[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Processing for Modeling\n",
    "- Train-Test split with a ratio of 9:1\n",
    "- Tokenising and Vectorising the 'text' column\n",
    "- Replacing 'text' with tokens columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a tokeniser with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# nlp = spacy.load('en')\n",
    "\n",
    "# Create our list of stopwords\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# parser = English()\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "#     mytokens = [ word.lower_.strip() for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>Due to memory issues, I was not able to Ngram 2 or more here.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24732, 4)\n",
      "(24732, 20079)\n"
     ]
    }
   ],
   "source": [
    "NGRAM = 1\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_wide_x, train_wide_y, test_size=0.1, \n",
    "                                                    random_state=42, stratify=train_wide_y)\n",
    "\n",
    "print(train_X.shape)\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,NGRAM))\n",
    "# vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "train_vec_X = vectorizer.fit_transform(train_X['text'])\n",
    "test_vec_X = vectorizer.transform(test_X['text'])\n",
    "\n",
    "print(train_vec_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare input dataframe for model.  Combining all transformed dataframes together\n",
    "def prepare_df_final(df, mat_vec, drop_col):\n",
    "    df_dropped = df.drop(drop_col, axis = 1)\n",
    "    df_vec = pd.DataFrame(mat_vec.toarray()).set_index(df_dropped.index)\n",
    "    final = pd.concat([df_dropped, df_vec], axis = 1)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 1min 26s, total: 3min 55s\n",
      "Wall time: 4min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_X = prepare_df_final(train_X, train_vec_X, ['text'])\n",
    "test_X = prepare_df_final(test_X, test_vec_X, ['text'])\n",
    "\n",
    "del train_wide_x\n",
    "del train_wide_y\n",
    "del train_vec_X\n",
    "del test_vec_X\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "## Important to manage memory usage\n",
    "train_X = scipy.sparse.csr_matrix(train_X.values)\n",
    "test_X = scipy.sparse.csr_matrix(test_X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks to ensure shape of datasets are consistent and within expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24732, 20082)\n",
      "(2748, 20082)\n",
      "(24732,)\n",
      "(2748,)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a generic function to train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def fitModel(model, train_X, test_X, train_y, test_y):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Model: {}\".format(type(model).__name__))\n",
    "\n",
    "    if type(model).__name__ == 'XGBClassifier':\n",
    "        model.fit(train_X.toarray(), train_y)\n",
    "    else:\n",
    "        model.fit(train_X, train_y)\n",
    "    \n",
    "    # Print accuracy, time and number of dimensions\n",
    "    print(\"The program took %.3f seconds to complete.\" % (time.time() - start_time))\n",
    "\n",
    "    # Cross validation score\n",
    "#     f1score = cross_val_score(model, train_X, train_y, cv=10, scoring=\"f1\")\n",
    "#     print(f1score)\n",
    "#     print(\"The 10-fold cross validation f1 score is %.3f\" % f1score)    \n",
    "    \n",
    "    # Measure the accuracy\n",
    "    accuracy = model.score(test_X, test_y)\n",
    "    print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "    \n",
    "    return model, accuracy\n",
    "\n",
    "models = []\n",
    "ACCURACY_THRESHOLD = 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training based on list of models below with default setting.  No tuning at this stage.  Objective is to compare how each model performs against one another.\n",
    "\n",
    "   1. Naive Bayes Model\n",
    "   2. Ridge Classifier\n",
    "   3. Logistic Regression\n",
    "   4. XGBoost\n",
    "   5. Random Forest\n",
    "   6. Decision Tree\n",
    "   7. Gradient Boosting\n",
    "   8. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MultinomialNB\n",
      "The program took 0.948 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb, accuracy = fitModel(MultinomialNB(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RidgeClassifier\n",
      "The program took 1.178 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "rc, accuracy = fitModel(RidgeClassifier(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "The program took 4.053 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr, accuracy = fitModel(LogisticRegression(random_state = 1), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "<span style=\"color:red\">This may take hours to run</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBClassifier\n",
      "The program took 13205.883 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.692\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb, accuracy = fitModel(XGBClassifier(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForestClassifier\n",
      "The program took 65.399 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc, accuracy = fitModel(RandomForestClassifier(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DecisionTreeClassifier\n",
      "The program took 9.126 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc, accuracy = fitModel(DecisionTreeClassifier(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(dtc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GradientBoostingClassifier\n",
      "The program took 283.471 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc, accuracy = fitModel(GradientBoostingClassifier(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVC\n",
      "The program took 251.189 seconds to complete.\n",
      "The accuracy of the classifier on the test set is 0.697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc, accuracy = fitModel(SVC(), train_X, test_X, train_y, test_y)\n",
    "\n",
    "if accuracy > ACCURACY_THRESHOLD:\n",
    "    models.append(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models that meet minimum accuracy threshold: 7\n"
     ]
    }
   ],
   "source": [
    "print('Number of models that meet minimum accuracy threshold: {}'.format(len(models)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Transformation for Test Dataset\n",
    "- Tokenising and Vectorising the 'text' column\n",
    "- Replacing 'text' with tokens columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec_text = vectorizer.transform(test['text'])\n",
    "test_final = prepare_df_final(test.drop(['sentiment'], axis=1), test_vec_text, ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3534, 20082)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final = scipy.sparse.csr_matrix(test_final.values)\n",
    "test_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Prediction and Results\n",
    "Show details of all models that will be used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                 max_iter=None, normalize=False, random_state=None,\n",
       "                 solver='auto', tol=0.001),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "               importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "               objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "               tree_method=None, validate_parameters=False, verbosity=None),\n",
       " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=None, splitter='best'),\n",
       " GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                            learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                            max_features=None, max_leaf_nodes=None,\n",
       "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                            n_iter_no_change=None, presort='deprecated',\n",
       "                            random_state=None, subsample=1.0, tol=0.0001,\n",
       "                            validation_fraction=0.1, verbose=0,\n",
       "                            warm_start=False),\n",
       " SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "     tol=0.001, verbose=False)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "predictions = []\n",
    "for model in models:\n",
    "    try:\n",
    "        if type(model).__name__ == 'XGBClassifier':\n",
    "            prediction = model.predict(test_final.toarray())\n",
    "        else:\n",
    "            prediction = model.predict(test_final)      \n",
    "        predictions.append(prediction)\n",
    "    except:\n",
    "        print(model)\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respective Model Results - Weighted F1 Score\n",
    "Random Forest Classifier performs best on out of sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weighted_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.667039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingClassifier</th>\n",
       "      <td>0.648697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.703477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.711881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.679198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.696089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.696794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Weighted_F1\n",
       "DecisionTreeClassifier         0.667039\n",
       "GradientBoostingClassifier     0.648697\n",
       "LogisticRegression             0.703477\n",
       "RandomForestClassifier         0.711881\n",
       "RidgeClassifier                0.679198\n",
       "SVC                            0.696089\n",
       "XGBClassifier                  0.696794"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "results = list(zip(models, predictions))\n",
    "labels = np.unique(test.sentiment)\n",
    "\n",
    "f1 = {}\n",
    "for result in results:\n",
    "    model = type(result[0]).__name__\n",
    "    score = f1_score(test.sentiment, result[1], labels=labels, average='weighted')\n",
    "    f1[model] = score\n",
    "    \n",
    "pd.DataFrame([f1], index=['Weighted_F1']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling the models\n",
    "Aggregating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = pd.DataFrame(predictions)\n",
    "predict_results = pd.Series(predict_results.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = test.copy()\n",
    "compare['predicted'] = predict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Weighted_F1</th>\n",
       "      <td>0.709599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Ensemble\n",
       "Weighted_F1  0.709599"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([f1_score(compare.sentiment, compare.predicted, average='weighted')], index=['Weighted_F1'], columns=['Ensemble'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix over the true (rows), predicted (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>615</td>\n",
       "      <td>341</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>170</td>\n",
       "      <td>1103</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>37</td>\n",
       "      <td>277</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  neutral  positive\n",
       "negative       615      341        45\n",
       "neutral        170     1103       157\n",
       "positive        37      277       789"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm =  confusion_matrix(compare.sentiment, compare.predicted, labels=labels)\n",
    "\n",
    "pd.DataFrame(cm, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score of respective labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.674712</td>\n",
       "      <td>0.700095</td>\n",
       "      <td>0.753582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    negative   neutral  positive\n",
       "F1  0.674712  0.700095  0.753582"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([f1_score(compare.sentiment, compare.predicted, average=None, labels=labels)], index=['F1'], columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Random Forest Classifier has the best accuracy performance in terms of F1 Score.  The next analysis will attempt to fine tune the parameters of the RF model to see whether the accuracy can be improved.\n",
    "\n",
    "In addition, due to memory issues, I was not able to create more features by increasing ngrams beyond 1.  I believe model accuracy will improve with higher ngrams.  I will explore ways to overcome this issue.  Maybe running this script on Google Colab will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
